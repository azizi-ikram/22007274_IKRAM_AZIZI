# -*- coding: utf-8 -*-
"""BankMarketing
```python
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/azizi-ikram/22007274_IKRAM_AZIZI/blob/main/BankMarketing.ipynb
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
yukeshmarudhasalam_bankmarketing_path = kagglehub.dataset_download('yukeshmarudhasalam/bankmarketing')

print('Data source import complete.')

"""## Final Task

### Subtask:
Summarize the entire analysis, including key findings from data exploration, model performance, and important insights about factors influencing term deposit subscriptions.

## Summary:

### Data Analysis Key Findings

*   **Data Preparation:** Duplicate rows were successfully removed from the dataset. The target variable 'y' was encoded from 'yes'/'no' to 1/0. All other categorical features were one-hot encoded, resulting in a dataset with 54 numerical columns ready for modeling.
*   **Data Splitting:** The processed dataset was successfully split into training and testing sets, with 32940 samples for training and 8236 samples for testing, ensuring a consistent class distribution through stratification.
*   **Model Performance (Accuracy):**
    *   XGBoost Classifier achieved the highest accuracy at 0.9149.
    *   Random Forest Classifier followed closely with an accuracy of 0.9117.
    *   Logistic Regression showed a competitive accuracy of 0.9098.
*   **Model Performance (ROC AUC):** All models demonstrated strong predictive power, with XGBoost generally outperforming the others. While specific AUC scores were generated, the overall trend indicated good differentiation between classes.
*   **Key Influencing Factors (Feature Importance):**
    *   **Commonly Important:** Both Random Forest and XGBoost models highlighted `duration` (call duration), economic indicators (`nr.employed`, `euribor3m`, `emp.var.rate`, `cons.conf.idx`), and client demographics/behavior (`age`, `campaign`) as top predictors.
    *   **XGBoost Specific:** `nr.employed` (number of employees) showed significantly high importance in the XGBoost model, alongside `duration` and `month_oct`.
    *   **Random Forest Specific:** `duration`, `euribor3m`, and `age` were among the highest-ranked features for the Random Forest model.

### Insights or Next Steps

*   **Targeted Campaign Strategy:** The high importance of `duration` suggests that the length of interaction during a call is a critical factor. Training sales representatives to extend meaningful conversations could increase subscription rates. Economic indicators like `nr.employed` and `euribor3m` also play a significant role, implying that campaigns might be more successful during favorable economic conditions.
*   **Campaign Timing:** The analysis revealed significant differences in success rates across months, with October showing higher efficiency despite lower contact volumes, and May showing high contact volumes but lower efficiency. This suggests optimizing campaign timing and strategy based on monthly trends.
*   **Model Optimization and Exploration:** While XGBoost performed best, further hyperparameter tuning for all models (especially XGBoost and Random Forest) could potentially yield even better performance. Additionally, exploring other advanced ensemble methods or neural networks might uncover more nuanced patterns.

**Reasoning**:
To understand the relationship between 'campaign' (number of contacts performed during this campaign) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'campaign' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how the number of contacts relates to the subscription outcome.
"""

correlation_campaign_y = df['campaign'].corr(df['y'])
print(f"Correlation between 'campaign' and 'y': {correlation_campaign_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='campaign', data=df, palette='viridis', hue='y', legend=False)
plt.title('Distribution of Campaign Contacts by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Number of Contacts during Campaign')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'campaign' distribution for 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'age' and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'age' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how client age relates to the subscription outcome.
"""

correlation_age_y = df['age'].corr(df['y'])
print(f"Correlation between 'age' and 'y': {correlation_age_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='age', data=df, palette='coolwarm', hue='y', legend=False)
plt.title('Distribution of Age by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Age')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'age' distribution for 'y' categories displayed.")

print("Top 5 Feature Importances for XGBoost Model:")
print(feature_importances_xgb.head(5))

"""**Reasoning**:
To understand the relationship between 'cons.price.idx' (consumer price index) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'cons.price.idx' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how this key economic indicator relates to the subscription outcome.
"""

correlation_cons_price_idx_y = df['cons.price.idx'].corr(df['y'])
print(f"Correlation between 'cons.price.idx' and 'y': {correlation_cons_price_idx_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='cons.price.idx', data=df, palette='viridis', hue='y', legend=False)
plt.title('Distribution of Consumer Price Index by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Consumer Price Index')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'cons.price.idx' distribution for 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'emp.var.rate' (employment variation rate) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'emp.var.rate' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how this key economic indicator relates to the subscription outcome.
"""

correlation_emp_var_rate_y = df['emp.var.rate'].corr(df['y'])
print(f"Correlation between 'emp.var.rate' and 'y': {correlation_emp_var_rate_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='emp.var.rate', data=df, palette='plasma', hue='y', legend=False)
plt.title('Distribution of Employment Variation Rate by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Employment Variation Rate')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'emp.var.rate' distribution for 'y' categories displayed.")

print("Feature Importances for XGBoost Model:")
print(feature_importances_xgb)

"""**Reasoning**:
To understand the influence of other months on the target variable 'y', I will iterate through the one-hot encoded month columns and create a count plot for each. This will visually represent the subscription rates (y=1) versus non-subscriptions (y=0) for contacts made in each specific month compared to all other months, providing insights into monthly campaign effectiveness.
"""

month_cols = [col for col in df.columns if 'month_' in col and col not in ['month_oct', 'month_may']]

for month_col in month_cols:
    plt.figure(figsize=(8, 6))
    sns.countplot(x=month_col, hue='y', data=df, palette='coolwarm')

    month_name = month_col.replace('month_', '').capitalize()
    plt.title(f'Distribution of Subscriptions by Contact Month: {month_name}')
    plt.xlabel(f'Contact Month is {month_name} (0: No, 1: Yes)')
    plt.ylabel('Count')
    plt.xticks([0, 1], ['Other Months', month_name])
    plt.legend(title='Subscribed', labels=['No', 'Yes'])
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

print("Count plots for remaining month distributions by 'y' categories displayed.")

"""**Reasoning**:
To directly compare the influence of 'month_oct' and 'month_may' on the target variable 'y', I will create a figure containing two count plots side-by-side. Each subplot will show the distribution of subscriptions (y=1) and non-subscriptions (y=0) for the respective month (1 if contact in that month, 0 otherwise). This visual comparison will highlight the differences in subscription rates and contact volumes for these two months.
"""

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot for month_oct
sns.countplot(x='month_oct', hue='y', data=df, palette='coolwarm', ax=axes[0])
axes[0].set_title('Distribution of Subscriptions by Contact Month: October')
axes[0].set_xlabel('Contact Month is October (0: No, 1: Yes)')
axes[0].set_ylabel('Count')
axes[0].set_xticks([0, 1], ['Other Months', 'October'])
axes[0].legend(title='Subscribed', labels=['No', 'Yes'])
axes[0].grid(axis='y', linestyle='--', alpha=0.7)

# Plot for month_may
sns.countplot(x='month_may', hue='y', data=df, palette='coolwarm', ax=axes[1])
axes[1].set_title('Distribution of Subscriptions by Contact Month: May')
axes[1].set_xlabel('Contact Month is May (0: No, 1: Yes)')
axes[1].set_ylabel('Count')
axes[1].set_xticks([0, 1], ['Other Months', 'May'])
axes[1].legend(title='Subscribed', labels=['No', 'Yes'])
axes[1].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

print("Side-by-side count plots comparing 'month_oct' and 'month_may' distributions by 'y' categories displayed.")

"""**Reasoning**:
To visualize the distribution of 'month_may' in relation to the target variable 'y', I will create a count plot. Since 'month_may' is a binary feature (1 if the contact month was May, 0 otherwise), a count plot will effectively show the number of subscriptions (y=1) and non-subscriptions (y=0) for both 'month_may' categories. This will help understand if May contacts have a different subscription rate.
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='month_may', hue='y', data=df, palette='coolwarm')
plt.title('Distribution of Subscriptions by Contact Month: May')
plt.xlabel('Contact Month is May (0: No, 1: Yes)')
plt.ylabel('Count')
plt.xticks([0, 1], ['Other Months', 'May'])
plt.legend(title='Subscribed', labels=['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Count plot showing 'month_may' distribution by 'y' categories displayed.")

"""**Reasoning**:
To visualize the distribution of 'month_oct' in relation to the target variable 'y', I will create a count plot. Since 'month_oct' is a binary feature (1 if the contact month was October, 0 otherwise), a count plot will effectively show the number of subscriptions (y=1) and non-subscriptions (y=0) for both 'month_oct' categories. This will help understand if October contacts have a different subscription rate.
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='month_oct', hue='y', data=df, palette='coolwarm')
plt.title('Distribution of Subscriptions by Contact Month: October')
plt.xlabel('Contact Month is October (0: No, 1: Yes)')
plt.ylabel('Count')
plt.xticks([0, 1], ['Other Months', 'October'])
plt.legend(title='Subscribed', labels=['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Count plot showing 'month_oct' distribution by 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'cons.conf.idx' (consumer confidence index) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'cons.conf.idx' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how this key economic indicator relates to the subscription outcome.
"""

correlation_cons_conf_idx_y = df['cons.conf.idx'].corr(df['y'])
print(f"Correlation between 'cons.conf.idx' and 'y': {correlation_cons_conf_idx_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='cons.conf.idx', data=df, palette='coolwarm', hue='y', legend=False)
plt.title('Distribution of Consumer Confidence Index by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Consumer Confidence Index')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'cons.conf.idx' distribution for 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'euribor3m' (3-month Euribor rate) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'euribor3m' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how this key economic indicator relates to the subscription outcome.
"""

correlation_euribor3m_y = df['euribor3m'].corr(df['y'])
print(f"Correlation between 'euribor3m' and 'y': {correlation_euribor3m_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='euribor3m', data=df, palette='cividis', hue='y', legend=False)
plt.title('Distribution of 3-month Euribor Rate by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('3-month Euribor Rate')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'euribor3m' distribution for 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'nr.employed' (number of employees) and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will provide a numerical measure of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'nr.employed' for clients who subscribed ('y'=1) versus those who did not ('y'=0), helping us to see how this economic indicator relates to the subscription outcome.
"""

correlation_nr_employed_y = df['nr.employed'].corr(df['y'])
print(f"Correlation between 'nr.employed' and 'y': {correlation_nr_employed_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='nr.employed', data=df, palette='magma', hue='y', legend=False)
plt.title('Distribution of Number of Employees by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Number of Employees')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'nr.employed' distribution for 'y' categories displayed.")

"""**Reasoning**:
To understand the relationship between 'duration' and the target variable 'y', I will first calculate their Pearson correlation coefficient. This will give us a numerical value indicating the strength and direction of their linear relationship. Then, I will create a box plot to visually compare the distribution of 'duration' for clients who subscribed ('y'=1) versus those who did not ('y'=0), providing a clearer picture of how 'duration' influences the outcome.
"""

correlation_duration_y = df['duration'].corr(df['y'])
print(f"Correlation between 'duration' and 'y': {correlation_duration_y:.4f}")

plt.figure(figsize=(8, 6))
sns.boxplot(x='y', y='duration', data=df, palette='viridis', hue='y', legend=False)
plt.title('Distribution of Duration by Target Variable (y)')
plt.xlabel('Subscribed to Term Deposit (0: No, 1: Yes)')
plt.ylabel('Duration of Last Contact (seconds)')
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("Box plot showing 'duration' distribution for 'y' categories displayed.")

"""بسم الله"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from tabulate import tabulate

# إعدادات العرض
sns.set(style='whitegrid', palette='muted', color_codes=True)

# النمذجة التنبؤية
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance

# تثبيت العشوائية
import random
random.seed(42)
np.random.seed(42)

# مكتبة الإحصاء والتحذيرات
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv(f'{yukeshmarudhasalam_bankmarketing_path}/bankmarketing.csv')
df.head()

df.tail()

df.describe().T.plot(kind='bar')

df.info()

df.isnull().sum()

df.duplicated().sum()

numeric_cols = df.select_dtypes(include=['number']).columns

# رسم التوزيع لكل عمود رقمي
for col in numeric_cols:
    sns.histplot(x=col, data=df, kde=True)
    plt.show()

for col in df:
    if df[col].dtype == 'O':
        sns.countplot(x=col,data=df)
        plt.show()

"""# Task
Remove duplicate rows from the dataset, then encode the categorical target variable 'y' to numerical format (0 and 1) and encode all other categorical features using appropriate numerical transformations.

## Remove Duplicate Rows

### Subtask:
Identify and remove duplicate rows from the dataset to ensure data quality and prevent biased analysis.

**Reasoning**:
Remove duplicate rows from the DataFrame `df` using `drop_duplicates()` and then verify the number of duplicated rows.
"""

df.drop_duplicates(inplace=True)
print(f"Number of duplicated rows after removal: {df.duplicated().sum()}")

"""**Reasoning**:
The previous code failed because the DataFrame `df` was not defined in the execution context. I need to re-load the `df` from the CSV file before attempting to remove duplicates.


"""

df=pd.read_csv(f'{yukeshmarudhasalam_bankmarketing_path}/bankmarketing.csv')
df.drop_duplicates(inplace=True)
print(f"Number of duplicated rows after removal: {df.duplicated().sum()}")

"""**Reasoning**:
The previous cell failed because the 'pd' (pandas) library was not defined in the current execution environment, despite being imported in an earlier cell. To fix this, I will re-import pandas in the current code block to ensure it's available before using `pd.read_csv` and `df.drop_duplicates`.


"""

import pandas as pd
df=pd.read_csv(f'{yukeshmarudhasalam_bankmarketing_path}/bankmarketing.csv')
df.drop_duplicates(inplace=True)
print(f"Number of duplicated rows after removal: {df.duplicated().sum()}")

"""**Reasoning**:
The previous code failed because the variable `yukeshmarudhasalam_bankmarketing_path` was not defined in the current execution context. To fix this, I will re-import `kagglehub` and redefine the path variable along with importing pandas and loading the dataframe.


"""

import kagglehub
yukeshmarudhasalam_bankmarketing_path = kagglehub.dataset_download('yukeshmarudhasalam/bankmarketing')
import pandas as pd
df=pd.read_csv(f'{yukeshmarudhasalam_bankmarketing_path}/bankmarketing.csv')
df.drop_duplicates(inplace=True)
print(f"Number of duplicated rows after removal: {df.duplicated().sum()}")

"""**Reasoning**:
The next step is to encode the target variable 'y' from categorical ('yes', 'no') to numerical (1, 0) to prepare it for machine learning models. I will use the `map` function for this binary conversion.


"""

df['y'] = df['y'].map({'yes': 1, 'no': 0})
print("Encoded target variable 'y':")
print(df['y'].value_counts())

"""**Reasoning**:
Now that the target variable 'y' has been encoded, I need to identify the remaining categorical features in the DataFrame `df`. I will select columns with the 'object' dtype (excluding 'y', which is already handled) to prepare them for encoding.


"""

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print(f"Remaining categorical columns to encode: {categorical_cols}")

"""**Reasoning**:
Now that the target variable has been encoded and the remaining categorical columns identified, I will apply one-hot encoding to these columns using `pd.get_dummies`. This method will convert each category into a binary column, making them suitable for machine learning models.


"""

df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)
print("DataFrame after one-hot encoding:")
print(df.head())
print(f"Number of columns after encoding: {df.shape[1]}")

"""## Analyze Feature Correlations

### Subtask:
Calculate and visualize the correlation matrix of the processed dataset. This will help understand the relationships between all features, including the encoded target variable, and identify potential multicollinearity.

**Reasoning**:
First, I will calculate the correlation matrix for the DataFrame `df` to quantify the relationships between all features. Then I will generate a heatmap of this matrix to visualize these correlations, which will help identify patterns, strong relationships, and potential multicollinearity.
"""

plt.figure(figsize=(20, 15))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of All Features', fontsize=20)
plt.show()
print("Correlation matrix heatmap displayed.")

"""**Reasoning**:
The previous code failed because `plt` was not defined. I need to import `matplotlib.pyplot` to use `plt` for plotting the heatmap.


"""

import matplotlib.pyplot as plt
sns.set(style='whitegrid', palette='muted', color_codes=True)

plt.figure(figsize=(20, 15))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of All Features', fontsize=20)
plt.show()
print("Correlation matrix heatmap displayed.")

"""**Reasoning**:
The previous code failed because `sns` was not defined. I need to import `seaborn` to use `sns` for plotting the heatmap.


"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style='whitegrid', palette='muted', color_codes=True)

plt.figure(figsize=(20, 15))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of All Features', fontsize=20)
plt.show()
print("Correlation matrix heatmap displayed.")

"""## Split Data for Modeling

### Subtask:
Divide the processed dataset into training and testing sets. This step is crucial for evaluating the performance of machine learning models on unseen data.

**Reasoning**:
First, I will separate the features (X) from the target variable (y). Then, I will import `train_test_split` and use it to divide the data into training and testing sets, ensuring a 80/20 split, reproducibility with `random_state=42`, and class distribution preservation with `stratify=y`. Finally, I will print the shapes of the resulting sets to confirm the split.
"""

X = df.drop('y', axis=1)
y = df['y']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""## Train Classification Models

### Subtask:
Train one or more classification models (e.g., Logistic Regression, Random Forest Classifier, or XGBoost Classifier) on the training data to predict the likelihood of a client subscribing to a term deposit.

**Reasoning**:
I will initialize and train a Logistic Regression model on the training data as the first step in training classification models.
"""

log_reg_model = LogisticRegression(random_state=42, solver='liblinear')
log_reg_model.fit(X_train, y_train)
print("Logistic Regression model trained successfully.")

"""**Reasoning**:
The previous code failed because `LogisticRegression` was not defined. I need to re-import `LogisticRegression` from `sklearn.linear_model`, `RandomForestClassifier` from `sklearn.ensemble`, and `xgboost` (as `xgb`) to initialize and train all specified classification models.


"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# Logistic Regression
log_reg_model = LogisticRegression(random_state=42, solver='liblinear')
log_reg_model.fit(X_train, y_train)
print("Logistic Regression model trained successfully.")

# Random Forest Classifier
rf_classifier_model = RandomForestClassifier(random_state=42, n_estimators=100)
rf_classifier_model.fit(X_train, y_train)
print("Random Forest Classifier model trained successfully.")

# XGBoost Classifier
xgb_classifier_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_classifier_model.fit(X_train, y_train)
print("XGBoost Classifier model trained successfully.")

"""**Reasoning**:
Now that the classification models (Logistic Regression, Random Forest, XGBoost) have been trained, the next crucial step is to evaluate their performance on the unseen test data. This involves making predictions and then calculating various classification metrics to understand how well each model generalizes.


"""

from sklearn.metrics import accuracy_score, classification_report

# Evaluate Logistic Regression model
y_pred_log_reg = log_reg_model.predict(X_test)
print("--- Logistic Regression Performance ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred_log_reg))

# Evaluate Random Forest Classifier model
y_pred_rf = rf_classifier_model.predict(X_test)
print("\n--- Random Forest Classifier Performance ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

# Evaluate XGBoost Classifier model
y_pred_xgb = xgb_classifier_model.predict(X_test)
print("\n--- XGBoost Classifier Performance ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))

"""## Evaluate Models and Visualize Performance

### Subtask:
Evaluate the performance of the trained classification models using appropriate metrics such as accuracy, precision, recall, F1-score, and visualize the results using tools like confusion matrices and ROC curves. Make sure to include legends where appropriate for clarity.

**Reasoning**:
I need to calculate and visualize the confusion matrix for each trained model to assess their classification performance visually. This involves using `confusion_matrix` to get the matrix and `seaborn.heatmap` to plot it.
"""

from sklearn.metrics import confusion_matrix

# Confusion Matrix for Logistic Regression
cm_log_reg = confusion_matrix(y_test, y_pred_log_reg)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_log_reg, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

# Confusion Matrix for Random Forest Classifier
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Random Forest Classifier')
plt.show()

# Confusion Matrix for XGBoost Classifier
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - XGBoost Classifier')
plt.show()

print("Confusion matrices for all models displayed.")

"""**Reasoning**:
Now that confusion matrices have been visualized, I need to calculate the ROC curve and AUC score for each model, then plot all ROC curves on a single figure to compare their performance. This involves getting probability scores, using `roc_curve` and `auc` from `sklearn.metrics`, and using `matplotlib.pyplot` for plotting.


"""

from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

# Logistic Regression ROC Curve
y_prob_log_reg = log_reg_model.predict_proba(X_test)[:, 1]
fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, y_prob_log_reg)
auc_log_reg = auc(fpr_log_reg, tpr_log_reg)
plt.plot(fpr_log_reg, tpr_log_reg, label=f'Logistic Regression (AUC = {auc_log_reg:.2f})')

# Random Forest Classifier ROC Curve
y_prob_rf = rf_classifier_model.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
auc_rf = auc(fpr_rf, tpr_rf)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})')

# XGBoost Classifier ROC Curve
y_prob_xgb = xgb_classifier_model.predict_proba(X_test)[:, 1]
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)
auc_xgb = auc(fpr_xgb, tpr_xgb)
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.2f})')

# Plotting random classifier baseline
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier (AUC = 0.50)')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend()
plt.grid(True)
plt.show()

print("ROC curves for all models displayed.")

"""## Visualize Feature Importance

### Subtask:
Extract and visualize the feature importances from one of the trained ensemble models (e.g., Random Forest or XGBoost) to understand which factors most significantly influence the prediction of term deposit subscription. Ensure all plots include legends.

**Reasoning**:
I will extract feature importances from the trained Random Forest and XGBoost models, create DataFrames from these importances mapped to feature names from `X_train.columns`, and then sort them in descending order to identify the most significant features for visualization.
"""

feature_importances_rf = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_classifier_model.feature_importances_
}).sort_values(by='importance', ascending=False)

feature_importances_xgb = pd.DataFrame({
    'feature': X_train.columns,
    'importance': xgb_classifier_model.feature_importances_
}).sort_values(by='importance', ascending=False)

print("Top 10 features for Random Forest:")
print(feature_importances_rf.head(10))
print("\nTop 10 features for XGBoost:")
print(feature_importances_xgb.head(10))

"""**Reasoning**:
Now that the feature importances have been extracted and sorted, I will create horizontal bar plots to visualize the top 15 most important features for both the Random Forest and XGBoost models. This will allow for a clear comparison of which features contribute most to each model's predictions.


"""

fig, axes = plt.subplots(1, 2, figsize=(20, 8))

# Plotting Random Forest Feature Importances
sns.barplot(x='importance', y='feature', data=feature_importances_rf.head(15), ax=axes[0], palette='viridis')
axes[0].set_title('Top 15 Feature Importances (Random Forest)', fontsize=16)
axes[0].set_xlabel('Feature Importance', fontsize=12)
axes[0].set_ylabel('Features', fontsize=12)

# Plotting XGBoost Feature Importances
sns.barplot(x='importance', y='feature', data=feature_importances_xgb.head(15), ax=axes[1], palette='plasma')
axes[1].set_title('Top 15 Feature Importances (XGBoost)', fontsize=16)
axes[1].set_xlabel('Feature Importance', fontsize=12)
axes[1].set_ylabel('Features', fontsize=12)

plt.tight_layout()
plt.show()
print("Top 15 feature importances for Random Forest and XGBoost models displayed.")
```
"""**Reasoning**:
The previous code generated `FutureWarning` messages because `palette` was used without `hue` in `sns.barplot`. To address this, I will modify the `sns.barplot` calls to assign the `y` variable (`feature`) to the `hue` argument and set `legend=False` as recommended by the warning, ensuring cleaner code and compliance with future `seaborn` versions.


"""

fig, axes = plt.subplots(1, 2, figsize=(20, 8))

# Plotting Random Forest Feature Importances
sns.barplot(x='importance', y='feature', data=feature_importances_rf.head(15), ax=axes[0], palette='viridis', hue='feature', legend=False)
axes[0].set_title('Top 15 Feature Importances (Random Forest)', fontsize=16)
axes[0].set_xlabel('Feature Importance', fontsize=12)
axes[0].set_ylabel('Features', fontsize=12)

# Plotting XGBoost Feature Importances
sns.barplot(x='importance', y='feature', data=feature_importances_xgb.head(15), ax=axes[1], palette='plasma', hue='feature', legend=False)
axes[1].set_title('Top 15 Feature Importances (XGBoost)', fontsize=16)
axes[1].set_xlabel('Feature Importance', fontsize=12)
axes[1].set_ylabel('Features', fontsize=12)

plt.tight_layout()
plt.show()
print("Top 15 feature importances for Random Forest and XGBoost models displayed.")

"""## Final Task

### Subtask:
Summarize the entire analysis, including key findings from data exploration, model performance, and important insights about factors influencing term deposit subscriptions.

## Summary:

### Data Analysis Key Findings

*   **Data Preparation:** Duplicate rows were successfully removed from the dataset. The target variable 'y' was encoded from 'yes'/'no' to 1/0. All other categorical features were one-hot encoded, resulting in a dataset with 54 numerical columns ready for modeling.
*   **Data Splitting:** The processed dataset was successfully split into training and testing sets, with 32940 samples for training and 8236 samples for testing, ensuring a consistent class distribution through stratification.
*   **Model Performance (Accuracy):**
    *   XGBoost Classifier achieved the highest accuracy at 0.9149.
    *   Random Forest Classifier followed closely with an accuracy of 0.9117.
    *   Logistic Regression showed a competitive accuracy of 0.9098.
*   **Model Performance (ROC AUC):** All models demonstrated strong predictive power, with XGBoost generally outperforming the others. While specific AUC scores were generated, the overall trend indicated good differentiation between classes.
*   **Key Influencing Factors (Feature Importance):**
    *   **Commonly Important:** Both Random Forest and XGBoost models highlighted `duration` (call duration), economic indicators (`nr.employed`, `euribor3m`, `emp.var.rate`, `cons.conf.idx`), and client demographics/behavior (`age`, `campaign`) as top predictors.
    *   **XGBoost Specific:** `nr.employed` (number of employees) showed significantly high importance in the XGBoost model, alongside `duration` and `month_oct`.
    *   **Random Forest Specific:** `duration`, `euribor3m`, and `age` were among the highest-ranked features for the Random Forest model.

### Insights or Next Steps

*   **Targeted Campaign Strategy:** The high importance of `duration` suggests that the length of interaction during a call is a critical factor. Training sales representatives to extend meaningful conversations could increase subscription rates. Economic indicators like `nr.employed` and `euribor3m` also play a significant role, implying that campaigns might be more successful during favorable economic conditions.
*   **Model Optimization and Exploration:** While XGBoost performed best, further hyperparameter tuning for all models (especially XGBoost and Random Forest) could potentially yield even better performance. Additionally, exploring other advanced ensemble methods or neural networks might uncover more nuanced patterns.
"""
